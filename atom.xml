<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[leonsbox.com]]></title>
  <link href="http://leonsbox.com/atom.xml" rel="self"/>
  <link href="http://leonsbox.com/"/>
  <updated>2013-06-05T10:50:58+00:00</updated>
  <id>http://leonsbox.com/</id>
  <author>
    <name><![CDATA[Leonid Bugaev]]></name>
    <email><![CDATA[leonsbox@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Improving testing by using real traffic from production]]></title>
    <link href="http://leonsbox.com/blog/2013/06/04/improving-testing-by-using-real-traffic-from-production/"/>
    <updated>2013-06-04T08:10:00+00:00</updated>
    <id>http://leonsbox.com/blog/2013/06/04/improving-testing-by-using-real-traffic-from-production</id>
    <content type="html"><![CDATA[<p><em>TL;DR The more realistic your test data, the better. I wrote <strong><a href="https://github.com/buger/gor/">Gor</a></strong> - an automated real-time http traffic replay solution that can forward part of your production traffic to staging or anywhere you want, and rate-limit it if needed</em></p>

<p>Here at <a href="http://granify.com">Granify</a> we work with a LOT of user generated data, our business is build on it. So our main priority is to ensure that we are collecting and processing it right.</p>

<p>You can&#8217;t imagine how weird user input can be; oddities come from many sources&#8212;proxies, browsers you&#8217;ve never heard of, client side bugs, and much more.</p>

<blockquote><p>No matter how many tests and fixtures you have, they just can&#8217;t cover all cases. Production traffic always differs from your expectations.</p></blockquote>


<p>Moreover, you can just brake everything when deploying a new version, even if tests pass. This happens constantly.</p>

<p>There is a whole layer of errors that just can&#8217;t be easily found via automated  and manual testing: concurrency, server environment specific bugs, some bugs can occur from requests called in a particular order, and etc.</p>

<p>We can do a few thing to make finding such bugs easier and improve overall stability:</p>

<h2>Testing on staging first</h2>

<p><a href="http://en.wikipedia.org/wiki/Staging_site">Staging</a> environment is mandatory, and it <b>should</b> be identical to production. Using tools like <a href="http://puppetlabs.com/">Puppet</a> or <a href="http://www.opscode.com/chef/">Chief</a> helps a lot.</p>

<p>You should require developers manually test all code on staging.</p>

<p>This helps you to find most obvious bugs, but it&#8217;s far from what you can get on production.</p>

<h2>Testing on production data</h2>

<p>There are a few techniques to do this (better to use both):</p>

<ol>
<li><p>Deploying changes only to one of production servers, so part of users will be served by the new code. This technique has some downsides. Some of your users will see errors/bugs, and you may have to use &#8220;sticky&#8221; sessions. This is quite similar to A/B testing.</p></li>
<li><p>Replaying production traffic to staging environment.</p></li>
</ol>


<p>Ilya Grigorik wrote a nice article on this: <a href="http://www.igvita.com/2008/09/30/load-testing-with-log-replay">Load Testing With Log Replay</a></p>

<p>In all articles I&#8217;ve read, log replay techniques are mostly mentioned in the context of load testing. I want to show you how to use log replay for daily bug testing.</p>

<p>Tools like <a href="http://jmeter.apache.org/">jMeter</a>, <a href="https://code.google.com/p/httperf/">httperf</a> or <a href="http://tsung.erlang-projects.org/">Tsung</a> have support for replaying logs, but it&#8217;s very rudimentary or focused on load testing and not emulating real-users. Feel the difference? Real users are not just a list of requests, they have proper timing between requests, different http headers etc. For load testing, it does not matter, but for bug testing it matters a lot.</p>

<p>Plus these tools require action and complex configuration to run.</p>

<blockquote><p>Developers are lazy. If you want your developers to use your tool/service it should be automated as much as possible, it&#8217;s best if no one notices it at all.</p></blockquote>


<p>Thats why we invented CI, automatic server configuration tools (Puppet, Chief, Fabric), EC2, and etc.</p>

<h2>Automating Log replay</h2>

<p>I wrote a simple tool <a href="https://github.com/buger/gor/">Gor</a></p>

<p>Gor allows you to automatically replicate traffic from production to staging (or anywhere you want) in real-time, 24 hours a day, with minimal effort. So your staging environment always gets part of production traffic.</p>

<p>Gor consists of 2 parts: Listener and Replay server. You should install Listener on your production web servers, and Replay server on a separate machine that will forward traffic to given address. The data flow is shown in the following diagram::</p>

<p><img src="https://a248.e.akamai.net/camo.github.com/c802ae10dfd1b0b2519c5726eedad31bac18c0f6/687474703a2f2f692e696d6775722e636f6d2f7a5a43465043592e706e67" alt="Diagram" /></p>

<p><strong>Gor supports rate limiting</strong>. This is a very important feature since staging environment usually uses fewer servers than production, and you&#8217;ll want to set maximum requests per second that staging can handle.</p>

<p>You can find all needed info on project <a href="https://github.com/buger/gor/">Github</a> page.</p>

<p>Since Gor is written in Go, everything is statically linked, so you can just download binaries from this link: <a href="https://drive.google.com/folderview?id=0B46uay48NwcfWFowc1E4a1BISVU&amp;usp=sharing">Downloads</a></p>

<p>At <a href="http://granify.com">Granify</a>, we use it on production for some time, and we are very happy with the results.</p>

<p>Happy testing!</p>

<hr />

<p>You can discuss this post at <a href="https://news.ycombinator.com/item?id=5824387">Hacker News</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Your code sucks]]></title>
    <link href="http://leonsbox.com/blog/2012/12/01/your-code-sucks/"/>
    <updated>2012-12-01T14:26:00+00:00</updated>
    <id>http://leonsbox.com/blog/2012/12/01/your-code-sucks</id>
    <content type="html"><![CDATA[<p>I know by myself, <strong>EVERY</strong> time when I get a new project, with existing code base, first thought is frustration, and desire to rewrite it the way it feels right. It works for all programmers without exception.</p>

<p>There is one thing I’ve learned so far: you need to stifle your inner grumbler. If you work in a team, you have to accept that things not always be as you want. Every developer has its own tricks. You have to trust and learn to delegate.</p>

<p>The only thing that matter is <strong>Business value</strong>. If you want to grow and not be just coder you need to understand this term.</p>

<p>Almost all project goes through same stages, and on each stage you have a different goal and ways to accomplish them. On earlier stages, it means speed of development and rapid prototyping. Further its challenges like growing and scaling team and product. If all goes well, someday you will rewrite all your code to Scala :)</p>

<p>Lack of documentation and some temporary code is <strong>OK</strong> on early stages; it don’t have to be perfect, it should be good enough. Things change if you need to jump higher and scale team. You have to think about ease of learning for newcomers, and support costs, because it becomes crucial for business value.</p>

<p>Remember Twitter story. When it started it was super buggy Rails app, that was down all the time. Now check where they now, they looks much more like enterprise, Java everywhere, 1500+ employees. Good project is like a book; you can’t skip and start reading from the middle, you need to pass all stages one-by-one.</p>

<p>Each project has a unique history, respect it, do not make hasty decisions, always think about business value.</p>

<h2>UPDATE</h2>

<p>For sure I don’t mean that you should write crappy code. I mostly talking about premature-optimizations and rewriting from scratch. Right now there are lot of tools and techniques that allow to control code quality. At least tests, code-reviews and code analysers.</p>

<p>Excellent article from Joel Spolsky <a href="http://www.joelonsoftware.com/articles/fog0000000069.html">Things You Should Never Do</a></p>

<hr />

<p>You can discuss this post at <a href="http://news.ycombinator.com/item?id=4857463">Hacker News</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Open letter to Airbrake.io]]></title>
    <link href="http://leonsbox.com/blog/2012/11/20/open-letter-to-airbrake-dot-io/"/>
    <updated>2012-11-20T13:26:00+00:00</updated>
    <id>http://leonsbox.com/blog/2012/11/20/open-letter-to-airbrake-dot-io</id>
    <content type="html"><![CDATA[<blockquote><p>TL;DR</p><p>If you provide solution for such critical development parts as error tracking, you should work all the time.</p></blockquote>


<p>Imagine that your site is down, you get lot of error, your business loses money. What can make you more angry? Oh, i say: when debug tool you rely on says that you reach limit of errors per minute, and just shows nothing, except this message. When you have multiple servers with multiple apps, digging through logs for errors without tool above becomes real pain.</p>

<p>Our load only about 30r/s, let&#8217;s imagine that something bad happened and  20 of this requests throwing errors. It will immediately reach your limits even on most expensive plan ( 1200 errors/minute - $999), so <strong>upgrading account will not help</strong>. And when you reach this limit, even if you fixed it in 15 minutes, you will get paralyzed for full day. I really want to give you my money, just give me reason.</p>

<h3>Suggestion for Airbrake team:</h3>

<ul>
<li><p>use soft limits, allow bursting. Just stopping working until user will upgrade plan (it will not help, ha ha), is not solution.</p></li>
<li><p>even if limit reached, you should allow view at least 1 error of each type, you can just disable some of functionality like similar errors.</p></li>
</ul>


<p>It&#8217;s a great product, i can see amazing insights, and it helps a lot during development. But it just did&#8217;t work when it is most needed.</p>

<p><strong>When shit happens we loosing more value than you provide.</strong></p>

<h3>Lessons learned</h3>

<p>Do not trust such important parts as debugging to external solution, always have plan B.</p>

<hr />

<p>You can discuss this post at <a href="http://news.ycombinator.com/item?id=4808539">Hacker News</a></p>
]]></content>
  </entry>
  
</feed>
